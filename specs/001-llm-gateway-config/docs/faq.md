# Frequently Asked Questions (FAQ)

**Common questions about LLM Gateway configuration for Claude Code.**

---

## General Questions

### Q: What is an LLM Gateway and why do I need one?

**A**: An LLM Gateway (like LiteLLM) sits between your application (Claude Code) and AI provider APIs. Benefits:

- **Model switching**: Change models without code changes\n- **Cost optimization**: Cache responses (40-70% savings)\n- **Multi-provider**: Use Anthropic, Bedrock, Vertex AI simultaneously\n- **Fallback/retry**: Automatic failover if provider fails\n- **Usage analytics**: Track spending and usage patterns\n- **Rate limiting**: Control API usage\n\n**When to use**:\n- You want to experiment with multiple models\n- Cost optimization is important\n- You need fallback/reliability\n- You want centralized control\n\n**When NOT to use**:\n- Simple single-provider setup\n- Minimum infrastructure\n- Just getting started\n\n**See**: `docs/deployment-patterns-comparison.md`\n\n### Q: Do I need to install anything to use this?\n\n**A**: Depends on your chosen pattern:\n\n**Direct Provider** (Pattern 1):\n- ✅ Nothing extra needed\n- Just set `ANTHROPIC_API_KEY`\n\n**Local Gateway** (Pattern 2):\n- Install LiteLLM: `pip install litellm`\n- Create config file\n- Start gateway\n\n**Corporate Proxy** (Pattern 3+):\n- Configure proxy environment variables\n- Install CA certificate (if SSL inspection)\n- May need VPN connection\n\n**See**: Setup guides in `examples/us1-quickstart-basic.md`, `examples/us4-corporate-proxy-setup.md`\n\n### Q: How much does this cost?\n\n**A**: The gateway itself is free (LiteLLM is open source). You only pay provider API costs:\n\n**Without Gateway**:\n- Every request costs money\n- No caching or optimization\n- Baseline cost: 100%\n\n**With Gateway** (caching enabled):\n- Cache hit rate: 30-60% typical\n- Cost savings: 40-70%\n- Example: $1000/month → $300-600/month\n\n**Infrastructure costs**:\n- Local gateway: Negligible (your laptop)\n- Shared gateway: $50-200/month (small server)\n\n**See**: `examples/us3-cost-optimization.md`\n\n---\n\n## Setup Questions\n\n### Q: How long does setup take?\n\n**A**: Depends on deployment pattern:\n\n| Pattern | Time | Complexity |\n|---------|------|------------|\n| Direct Provider | 5 min | Very Easy |\n| Local Gateway | 10-15 min | Easy |\n| Corporate Proxy | 15-20 min | Medium |\n| Proxy + Gateway | 20-30 min | Medium |\n| Enterprise Gateway | 30-60 min | Hard |\n\n**See**: `docs/deployment-patterns-comparison.md`\n\n### Q: Can I use this without a gateway?\n\n**A**: Yes! Direct provider access works fine:\n\n```bash\n# Just set API key, no gateway needed\nexport ANTHROPIC_API_KEY=\"sk-ant-api03-...\"\nanthropic-code  # Works directly\n```\n\nGateway is optional but recommended for flexibility and cost savings.\n\n### Q: Do I need Docker?\n\n**A**: No. LiteLLM runs as a Python process. Docker is optional:\n\n**Without Docker**:\n```bash\npip install litellm\nlitellm --config config.yaml --port 4000\n```\n\n**With Docker** (optional):\n```bash\ndocker run -p 4000:4000 -v $(pwd)/config.yaml:/app/config.yaml \\\n  ghcr.io/berriai/litellm:latest --config /app/config.yaml\n```\n\n**See**: `examples/docker/` (coming in Phase 7)\n\n---\n\n## Configuration Questions\n\n### Q: Where do I put configuration files?\n\n**A**: Recommended locations:\n\n```bash\n# User-specific\n~/.claude/litellm-config.yaml\n~/.litellm/config.yaml\n\n# Project-specific\n$PROJECT_ROOT/config/litellm.yaml\n./litellm-config.yaml\n\n# System-wide\n/etc/litellm/config.yaml\n`` \n\nUse templates from `templates/` directory as starting point.\n\n### Q: How do I add a new model?\n\n**A**: Add to `model_list` in your config file:\n\n ``yaml\nmodel_list:\n # Existing models...\n \n # Add new model\n - model_name: my-new-model\n litellm_params:\n model: vertex_ai/gemini-2.0-flash-exp\n vertex_project: os.environ/VERTEX_PROJECT_ID\n vertex_location: us-central1\n`\n\nRestart gateway:\n`bash\npkill litellm\nlitellm --config config.yaml --port 4000\n`\n\nVerify:\n`bash\ncurl http://localhost:4000/models | jq\n`\n\n### Q: Can I use multiple providers simultaneously?\n\n**A**: Yes! Configure all providers in one config:\n\n`yaml\nmodel_list:\n # Anthropic\n - model_name: claude-sonnet\n litellm_params:\n model: anthropic/claude-3-5-sonnet-20241022\n api_key: os.environ/ANTHROPIC_API_KEY\n \n # AWS Bedrock\n - model_name: bedrock-claude\n litellm_params:\n model: bedrock/anthropic.claude-3-5-sonnet-20241022-v2:0\n \n # Google Vertex AI\n - model_name: gemini-flash\n litellm_params:\n model: vertex_ai/gemini-2.0-flash-exp\n vertex_project: os.environ/VERTEX_PROJECT_ID\n`` \n\n**See**: `examples/us3-multi-provider-setup.md`\n\n### Q: How do I switch between models?\n\n**A**: Use the `--model` flag or set default:\n\n ``bash\n# Use specific model\nclaude --model gemini-flash \"What is 2+2?\"\nclaude --model claude-sonnet \"What is 2+2?\"\n\n# Set default in config\nexport ANTHROPIC_DEFAULT_MODEL=\"gemini-flash\"\n`\n\n---\n\n## Corporate/Enterprise Questions\n\n### Q: Will this work behind my corporate firewall?\n\n**A**: Yes! Configure corporate proxy:\n\n`bash\nexport HTTPS_PROXY=\"http://proxy.corp.example.com:8080\"\nexport NO_PROXY=\"localhost,127.0.0.1,.internal\"\n`` \n\nGateway automatically uses proxy for external requests.\n\n**See**: `examples/us4-corporate-proxy-setup.md`\n\n### Q: My proxy requires authentication. How do I configure that?\n\n**A**: Include credentials in proxy URL:\n\n ``bash\n# Basic format\nexport HTTPS_PROXY=\"http://username:password@proxy:8080\"\n\n# URL-encode special characters\npython3 -c \"import urllib.parse; print(urllib.parse.quote('p@ss:w/rd!', safe=''))\"\n# Output: p%40ss%3Aw%2Frd%21\n\nexport HTTPS_PROXY=\"http://username:p%40ss%3Aw%2Frd%21@proxy:8080\"\n`` \n\n**See**: `examples/us4-https-proxy-config.md#proxy-authentication`\n\n### Q: Do I need to install a certificate?\n\n**A**: Only if your corporate proxy performs SSL inspection:\n\n**Check if needed**:\n ``bash\ncurl -v https://api.anthropic.com 2>&1 | grep 'issuer'\n# If issuer is your company → certificate needed\n`\n\n**Install**:\n`bash\n# Get certificate from IT\nsudo cp corporate-ca-bundle.crt /usr/local/share/ca-certificates/\nsudo update-ca-certificates\n\n# Or set environment variable\nexport SSL_CERT_FILE=\"/path/to/corporate-ca-bundle.crt\"\n`` \n\n**See**: `examples/us4-corporate-proxy-setup.md#step-2-install-ca-certificate`\n\n### Q: Can multiple people share one gateway?\n\n**A**: Yes! Deploy gateway on shared server:\n\n ``bash\n# On server\nlitellm --config config.yaml --host 0.0.0.0 --port 4000\n\n# All team members\nexport ANTHROPIC_BASE_URL=\"http://gateway.internal.corp:4000\"\n```\n\n**Benefits**:\n- Shared cache (better cost savings)\n- Centralized management\n- Consistent configuration\n\n**See**: `examples/us2-enterprise-integration.md`\n\n---\n\n## Security Questions\n\n### Q: Is my API key safe?\n\n**A**: If configured correctly, yes:\n\n**✅ Safe**:\n- Stored in environment variables\n- Stored in secrets manager (AWS Secrets Manager, 1Password)\n- Used with gateway (Claude Code never sees real key)\n\n**❌ Unsafe**:\n- Hardcoded in files\n- Committed to Git\n- Shared via email/chat\n- Visible in scripts/logs\n\n**Best practice**:\n`bash\n# Use secrets manager\nexport ANTHROPIC_API_KEY=$(aws secretsmanager get-secret-value --secret-id anthropic-key --query SecretString --output text)\n\n# Or 1Password CLI\nexport ANTHROPIC_API_KEY=$(op read \"op://Private/Anthropic/credential\")\n`\n\n**See**: `docs/security-best-practices.md#api-key-security`\n\n### Q: Does the gateway log my prompts?\n\n**A**: Depends on configuration:\n\n**Default behavior**:\n- Gateway logs metadata (model, tokens, latency)\n- Gateway does NOT log prompt/response content\n\n**To verify**:\n`bash\ngrep -E '(prompt|message|content)' ~/.litellm/logs/litellm.log\n# Should not show prompt content\n`\n\n**Corporate proxy** may log everything if SSL inspection enabled.\n\n**See**: `docs/security-best-practices.md#logging-monitoring`\n\n### Q: Can I use this with sensitive data?\n\n**A**: Yes, with proper controls:\n\n**Pre-flight validation** (recommended):\n`python\n# Check for PII before sending\nimport re\n\ndef validate_prompt(prompt):\n    # Check for SSN\n    if re.search(r'\\b\\d{3}-\\d{2}-\\d{4}\\b', prompt):\n        raise ValueError(\"Prompt contains SSN\")\n    # Check for API keys\n    if re.search(r'sk-ant-api\\w+', prompt):\n        raise ValueError(\"Prompt contains API key\")\n    return True\n`\n\n**DLP at proxy** (if SSL inspection enabled):\n- Blocks sensitive patterns\n- Logs violations\n- Alerts security team\n\n**See**: `docs/security-best-practices.md#data-protection`, `examples/us2-security-best-practices.md`\n\n---\n\n## Performance Questions\n\n### Q: Will a gateway make requests slower?\n\n**A**: Slightly slower for uncached, much faster for cached:\n\n**Latency**:\n- Direct: ~400ms (baseline)\n- Gateway (uncached): ~420ms (+20ms overhead)\n- Gateway (cached): ~50ms (90% faster!)\n\n**With proxy**:\n- Proxy adds ~20-50ms\n- Total: ~470ms uncached, ~70ms cached\n\n**Cache hit rate**: 30-60% typical → most requests are faster\n\n**See**: `docs/deployment-patterns-comparison.md#performance-comparison`\n\n### Q: How much memory does the gateway use?\n\n**A**: Depends on cache configuration:\n\n**Minimal** (no cache):\n- ~100-200 MB RAM\n\n**With Redis cache** (recommended):\n- Gateway: ~200-500 MB\n- Redis: ~500-2000 MB (depends on cache size)\n- Total: ~700-2500 MB\n\n**Optimization**:\n`yaml\ncache_params:\n  max_size_mb: 1000  # Limit cache to 1GB\n  ttl: 3600  # 1 hour TTL\n`\n\n### Q: Can the gateway handle multiple users?\n\n**A**: Yes! LiteLLM is production-ready:\n\n**Tested scale**:\n- 100+ concurrent users\n- 1000+ requests/minute\n- Millions of requests/day\n\n**For high load**:\n- Use Redis for caching (not in-memory)\n- Run multiple gateway instances (load balanced)\n- Monitor memory/CPU usage\n\n---\n\n## Cost Questions\n\n### Q: How much money can caching save?\n\n**A**: Significant savings:\n\n**Example scenario** (100 developers):\n- Usage: 1M tokens/day per user = 100M tokens/day total\n- Cost without cache: $1,500,000/month\n- Cache hit rate: 40% (conservative)\n- Cost with cache: $900,000/month\n- **Savings: $600,000/month (40%)**\n\n**With optimizations**:\n- Cache hit rate: 60% (well-tuned)\n- Cost: $600,000/month\n- **Savings: $900,000/month (60%)**\n\n**See**: `examples/us3-cost-optimization.md`\n\n### Q: Which model is cheapest?\n\n**A**: Cost per 1M tokens (as of 2024):\n\n| Model | Input | Output | Total (avg) |\n|-------|-------|--------|-------------|\n| Gemini 2.0 Flash | $0.10 | $0.30 | ~$0.20 |\n| Claude 3.5 Haiku | $1.00 | $5.00 | ~$3.00 |\n| Claude 3.5 Sonnet | $3.00 | $15.00 | ~$9.00 |\n| Gemini 2.5 Pro | $1.25 | $5.00 | ~$3.13 |\n| Claude 3 Opus | $15.00 | $75.00 | ~$45.00 |\n\n**Recommendation**: Use Gemini 2.0 Flash for most tasks, Claude Sonnet for complex reasoning.\n\n### Q: Does the gateway itself cost money?\n\n**A**: LiteLLM is free and open source. Only costs:\n\n**Infrastructure** (if shared gateway):\n- Small server: $50-100/month\n- Medium server: $100-200/month\n- Redis: $20-50/month (optional)\n\n**Provider APIs**: Pay-per-use based on tokens\n\n**Total**: Infrastructure (~$150/month) + API usage\n\n---\n\n## Troubleshooting Questions\n\n### Q: Gateway won't start. What do I check?\n\n**A**: Common issues:\n\n1. **Config syntax error**:\n `bash\n   python3 -c \"import yaml; yaml.safe_load(open('config.yaml'))\"\n   `\n\n2. **Port already in use**:\n `bash\n   lsof -i :4000\n   pkill -9 litellm  # Kill existing\n   `\n\n3. **Missing environment variables**:\n `bash\n   python scripts/validate-provider-env-vars.py\n   `\n\n4. **Permission denied**:\n `bash\n   # Don't use sudo unless necessary\n   litellm --config config.yaml --port 4000\n   `\n\n**See**: `docs/troubleshooting-guide.md#gateway-wont-start`\n\n### Q: I get \"401 Unauthorized\". What's wrong?\n\n**A**: API key issue. Check:\n\n1. **Is key set?**\n `bash\n   echo $ANTHROPIC_API_KEY\n   `\n\n2. **Correct format?**\n - Anthropic: starts with`sk-ant-`\n - AWS:`AKIA...` (20 chars)\n\n3. **Test key directly**:\n `bash\n   curl https://api.anthropic.com/v1/messages \\\n     -H \"x-api-key: $ANTHROPIC_API_KEY\" \\\n     -H \"anthropic-version: 2023-06-01\" \\\n     -H \"content-type: application/json\" \\\n     -d '{\"model\":\"claude-3-5-sonnet-20241022\",\"max_tokens\":10,\"messages\":[{\"role\":\"user\",\"content\":\"Hi\"}]}'\n   `\n\n4. **Regenerate key** if still failing\n\n**See**: `docs/troubleshooting-guide.md#authentication-issues`\n\n### Q: Requests are timing out. How do I fix this?\n\n**A**: Increase timeout:\n\n`yaml\n# config.yaml\nlitellm_settings:\n  request_timeout: 600  # 10 minutes\n  num_retries: 3\n  retry_after: 10\n`\n\nAlso check:\n- Proxy latency: `time curl -x $HTTPS_PROXY https://httpbin.org/ip`\n- Provider status: https://status.anthropic.com\n- Network bandwidth\n\n---\n\n## Migration Questions\n\n### Q: Can I migrate from direct to gateway without downtime?\n\n**A**: Yes! Gradual migration:\n\n**Step 1**: Set up gateway (doesn't affect existing setup)\n`bash\nlitellm --config config.yaml --port 4000\n`\n\n**Step 2**: Test with new terminal session\n`bash\n# New terminal\nexport ANTHROPIC_BASE_URL=\"http://localhost:4000\"\nclaude \"test\"  # Uses gateway\n`\n\n**Step 3**: Original terminal still works directly\n`bash\n# Original terminal (no ANTHROPIC_BASE_URL)\nclaude \"test\"  # Uses direct API\n`\n\n**Step 4**: When confident, update permanently\n`bash\necho 'export ANTHROPIC_BASE_URL=\"http://localhost:4000\"' >> ~/.bashrc\n`\n\n**Rollback**: Just unset `ANTHROPIC_BASE_URL`\n\n### Q: How do I switch from one provider to another?\n\n**A**: Easy with gateway:\n\n`bash\n# Just change model name\nclaude --model gemini-flash \"question\"  # Use Vertex AI\nclaude --model claude-sonnet \"question\"  # Use Anthropic\nclaude --model bedrock-claude \"question\"  # Use Bedrock\n\n# Or change default\nexport ANTHROPIC_DEFAULT_MODEL=\"gemini-flash\"\n`\n\nNo code changes needed!\n\n---\n\n## Getting Help\n\n### Q: Where can I find more documentation?\n\n**A**: Documentation structure:\n\n`\nspecs/001-llm-gateway-config/\n├── docs/                    # Reference documentation\n│   ├── configuration-reference.md\n│   ├── deployment-patterns-comparison.md\n│   ├── environment-variables.md\n│   ├── security-best-practices.md\n│   ├── troubleshooting-guide.md\n│   └── faq.md (this file)\n├── examples/                # Step-by-step guides\n│   ├── us1-quickstart-basic.md\n│   ├── us2-enterprise-integration.md\n│   ├── us3-multi-provider-setup.md\n│   └── us4-corporate-proxy-setup.md\n├── templates/               # Configuration templates\n│   ├── litellm-complete.yaml\n│   ├── models/\n│   ├── enterprise/\n│   ├── multi-provider/\n│   └── proxy/\n└── scripts/                 # Helper scripts\n    ├── validate-config.py\n    ├── health-check.sh\n    └── ...\n`\n\n### Q: Who do I contact for help?\n\n**A**:\n\n**Technical Issues**:\n- LiteLLM: https://docs.litellm.ai/\n- GitHub Issues: https://github.com/BerriAI/litellm/issues\n\n**Provider Issues**:\n- Anthropic: https://support.anthropic.com/\n- AWS Bedrock: AWS Support Console\n- Google Vertex: https://cloud.google.com/support/\n\n**Corporate/Network Issues**:\n- IT Helpdesk (proxy, VPN, certificates)\n- Security Team (compliance, authentication)\n- DevOps Team (infrastructure, deployment)\n\n### Q: Can I contribute improvements to this documentation?\n\n**A**: Yes! This is open documentation. To contribute:\n\n1. Fork the repository\n2. Make changes in `specs/001-llm-gateway-config/`\n3. Test your changes\n4. Submit pull request\n5. Tag relevant reviewers\n\n---\n\n## Quick Reference\n\n### Essential Commands\n\n`bash\n# Start gateway\nlitellm --config config.yaml --port 4000\n\n# Test gateway\ncurl http://localhost:4000/health\n\n# List models\ncurl http://localhost:4000/models\n\n# Test completion\nclaude \"What is 2+2?\"\n\n# Check logs\ntail -f ~/.litellm/logs/litellm.log\n\n# Run tests\npython tests/test-all-models.py\n`\n\n### Essential Files\n\n- **Quickstart**: `examples/us1-quickstart-basic.md`\n- **Corporate Setup**: `examples/us4-corporate-proxy-setup.md`\n- **Troubleshooting**: `docs/troubleshooting-guide.md`\n- **Configuration**: `docs/configuration-reference.md`\n\n---\n\n**Last Updated**: 2025-12-01 \n**Version**: 1.0.0\n\n**Have more questions?\*\* Check `docs/troubleshooting-guide.md` or contact support.\n
